# -*- coding: utf-8 -*-
"""classifier_zeroshot_tracking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15mDDi8VSjfED7GWZuIx8AVZQrWGR8OBd
"""
print("hi")
from scipy import signal
import scipy
import torch
from torch import nn
from torch.nn import Linear, Conv2d
from torch.nn import ReLU, LeakyReLU, PReLU, Softmax, Sigmoid, Tanh, ELU
from torch.functional import F
import scipy.io as sio
import numpy as np
import math
from sklearn.model_selection import KFold
import os
from tqdm import tqdm
import pickle
import pdb
import mat73
from datetime import datetime

# tf.debugging.set_log_device_placement(True)
def ext_spectrogram(epoch, fs=1000, window='hamming', nperseg=2000, noverlap=1975, nfft=3000):
    # epoch.shape = channel number, timepoint, trials
    # extract sepctrogram with time point

    dat = []
    for i in range(epoch.shape[2]):
        tfreq = []
        for j in range(epoch.shape[0]):
            f, t, Sxx = signal.stft(epoch[j, :, i], fs=fs, window=window, nperseg=nperseg, noverlap=noverlap, nfft=nfft)
            interval = f[-1] / (len(f) - 1)
            req_len = int(40 / interval)
            # use frequency(~121th) and tiem(-41th~0)
            tfreq.append(np.abs(Sxx[:121, -41:]).transpose())
            # tfreq.append(np.abs(Sxx[:121, :]).transpose())

        dat.append(np.asarray(tfreq))

    return np.array(dat)  # shape : (trials, channel number, time, freq), time and freq should be : 41, 121


def get_batch_num(data, batch_size):
    total_len = data.shape[0]
    return math.ceil(total_len / batch_size)


def get_batch(data, batch_size, idx):
    batch_num = get_batch_num(data, batch_size)
    if idx == batch_num - 1:  # last batch
        return data[batch_size * idx:]
    else:
        return data[batch_size * idx:batch_size * (idx + 1)]


# input: location
# output: train_x, train_y, test_x, test_y

def load_data_labels(location='dataset_original2.mat'):
    # load eeg data
    try:
        data = sio.loadmat(location)
    except:
        data = mat73.loadmat(location)

    # get eeg data and extract spectogram
    # reshpae spectogram data as shape (num_trials, features) to use it in FC
    ep = ext_spectrogram(data['ep']).reshape(data['ep'].shape[2],-1)
    # get label data
    # reshape it to (num_trials, 1) to use it in FC
    lb_maxrel = data['lb_maxrel'].T
    lb_pmb28 = data['lb_pmb28'].T
    lb_pmb37 = data['lb_pmb37'].T
    lb_act = data['lb_act'].T

    # check shape of ep & lb
    # print(ep.shape, lb.shape)
    # generate random index, for unbiased dataset
    # shuffle_idx = np.arange(ep.shape[0])
    # np.random.shuffle(shuffle_idx)
    # shuffle ep and lb in the same order
    np.random.seed(2121)
    shuffle_idx = np.random.permutation(lb_maxrel.shape[0])


    return ep[shuffle_idx], lb_maxrel[shuffle_idx], lb_pmb28[shuffle_idx], lb_pmb37[shuffle_idx], lb_act[shuffle_idx]



def load_data(location='dataset_original2.mat',is_total = False):
    # load eeg data
    data = sio.loadmat(location)
    # get eeg data and extract spectogram
    # reshpae spectogram data as shape (num_trials, features) to use it in FC
    ep = ext_spectrogram(data['ep']).reshape(data['ep'].shape[2], -1)
    # get label data
    # reshape it to (num_trials, 1) to use it in FC
    lb = data['lb'].T
    # check shape of ep & lb
    # print(ep.shape, lb.shape)
    # generate random index, for unbiased dataset
    # shuffle_idx = np.arange(ep.shape[0])
    # np.random.shuffle(shuffle_idx)
    # shuffle ep and lb in the same order
    if is_total:
        np.random.seed(2121)
        shuffle_idx = np.random.permutation(lb.shape[0])
        return ep[shuffle_idx], lb[shuffle_idx]
    else:
        shuffle_idx = np.random.permutation(lb.shape[0])
        ep = ep[shuffle_idx]
        lb = lb[shuffle_idx]
        num_train = int(ep.shape[0] * 9 / 10)
        return ep[:num_train], lb[:num_train], ep[num_train:], lb[num_train:]


class torch_net(nn.Module):
    def __init__(self, num_input):
        super(torch_net, self).__init__()
        self.num_input = num_input
        self.layers = nn.ModuleList()
        temp_list = [self.num_input, 4096, 2048, 1024, 512, 256, 128, 64, 32, 16, 8, 2]
        layers_in_list  = temp_list[:-1]
        layers_out_list = temp_list[1:]
        for i in range(len(layers_in_list)):
            self.layers.append(nn.Linear(layers_in_list[i], layers_out_list[i]))
            self.layers.append(nn.Tanh())

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        F.softmax(x, dim=1)
        return x



##### main
print("main")
save_all = False

if save_all:
    eps = []
    lbs = []
    for dataset in [0,1,5]:
        # for dataset in [0]:
        if not os.path.exists(f'./dataset{dataset}'):
            os.mkdir(f'./dataset{dataset}')
        print(f'dataset{dataset}')
        ep_tot, lb_tot = load_data(f'dataset{dataset}_parsed.mat', is_total=True)
        eps.append(ep_tot)
        lbs.append(lb_tot)

print("loading...")
ep_tots = []
lb_tots = []
strings_ = "./logs4_"+datetime.today().strftime('%Y%m%d-%H%M') +"/"
# for subi in [0,1,2,4,5,6,7,8,9,10,11,12,13,14,16,18,19,21,24,25,26,27,28,30]:
# for subi in range(33):
for subi in [0]:
    ep_tots_, lb_maxrel_tot, lb_pmb28_tot, lb_pmb37_tot, lb_act_tot = load_data_labels(
        './dat_sub/sub{0}.mat'.format(subi+1))  # original2
    ep_tots_ = (ep_tots_.reshape(ep_tots_.shape[0],16,-1))[:,:4,:]
    ep_tots_ = ep_tots_.reshape(ep_tots_.shape[0],-1)
    if len(ep_tots) == 0:
        ep_tots = ep_tots_
    else:
        ep_tots = np.concatenate((ep_tots, ep_tots_),axis=0)
    if len(lb_tots) == 0:
        lb_tots.append(lb_maxrel_tot)
        lb_tots.append(lb_pmb28_tot)
        lb_tots.append(lb_pmb37_tot)
        lb_tots.append(lb_act_tot)
    else:
        lb_tots[0]=np.concatenate((lb_tots[0],lb_maxrel_tot),axis=0)
        lb_tots[1]=np.concatenate((lb_tots[1],lb_pmb28_tot),axis=0)
        lb_tots[2]=np.concatenate((lb_tots[2],lb_pmb37_tot),axis=0)
        lb_tots[3]=np.concatenate((lb_tots[3],lb_act_tot),axis=0)



strings_="./logs"  +datetime.today().strftime('%Y%m%d-%H%M')+ "_subgroup_4ch/"
kf = KFold(n_splits=10, shuffle=False)
for lbi in [0]:
    lb_tot = lb_tots[lbi]
    lb1idx = np.where(lb_tot == 0)[0]
    lb2idx = np.where(lb_tot == 1)[0]
    minidx=min(lb1idx.shape[0],lb2idx.shape[0])
    lb1idx = lb1idx[:minidx]
    lb2idx = lb2idx[:minidx]
    lb_tot = np.concatenate((lb_tot[lb1idx],lb_tot[lb2idx]))
    ep_tot = np.concatenate((ep_tots[lb1idx,:],ep_tots[lb2idx,:]),axis=0)


    c = -1/(np.sqrt(2)*scipy.special.erfcinv(3/2))
    mad_ = c*np.median(np.abs(ep_tot-np.median(ep_tot,axis=1).reshape(-1,1)),axis=1)
    ep_tot = ep_tot[np.where(mad_<3)[0],:]
    lb_tot = lb_tot[np.where(mad_<3)[0]]


    np.random.seed(2020)
    index = np.random.permutation(ep_tot.shape[0])
    ep_tot = ep_tot[index, :]
    lb_tot = lb_tot[index]

    kf.get_n_splits(lb_tot)
    cv = 0
    batch_size = 22

    print("main2")
    for train_ind, test_ind in kf.split(lb_tot):
        ep, lb = ep_tot[train_ind], lb_tot[train_ind]
        test_x, test_y = ep_tot[test_ind], lb_tot[test_ind]
        cv += 1
        network = torch_net(num_input=ep.shape[1])
        # acc = []
        # loss = []
        infoxinfo = []
        epoch = []
        summary = {}
        if not os.path.exists('./cv{0}'.format(cv)):
            os.mkdir('./cv{0}'.format(cv))

        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(network.parameters(), lr=0.001)

        batch_num = get_batch_num(ep, batch_size)

        epochs = []
        costs = []
        for epoch in tqdm(range(50)):

            total_cost = 0
            ####part
            for i in range(batch_num):
                network.zero_grad()
                batch_ep = get_batch(ep, batch_size, i)
                batch_lb = get_batch(lb, batch_size, i)
                batch_ep = torch.Tensor(batch_ep)
                batch_lb = torch.Tensor(batch_lb).long()
                output = network(batch_ep)

                # batch_lb to one-hot vector
                batch_lb = torch.zeros(batch_lb.size(0), 2).scatter_(1, batch_lb.view(-1, 1), 1)

                loss = criterion(output, batch_lb)

                loss.backward()
                optimizer.step()
                total_cost += loss.item()

            costs.append(total_cost)
            epochs.append(epoch)
            print('Epoch [%d/%d], Loss: %.4f' % (epoch + 1, 50, total_cost))

        try:
            del ep, test_x, lb, test_y
        except:
            ''